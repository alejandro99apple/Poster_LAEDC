<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>ALS Detection Poster</title>
  <link rel="stylesheet" href="style.css">
  <link rel="icon" type="image/x-icon" href="UDG.ico">
</head>
<body>

  <header>
    <h1 style="display: flex; align-items: center; justify-content: center; gap: 12px; flex-wrap: wrap;">
      ALS Detection using Frequency Analysis with Machine Learning
    </h1>
    <p>
      <a href="https://www.linkedin.com/in/alejandrodiazmonstesdeoca/" class="author-link" style="color:inherit; text-decoration:none;">Alejandro Díaz-Montes-de-Oca</a>,
      <a href="https://scholar.google.com/citations?user=sXvbhC8AAAAJ&hl=es" class="author-link" style="color:inherit; text-decoration:none;">Ricardo A. Salido-Ruiz</a> and
      <a href="https://scholar.google.com/citations?user=Ypkf2-4AAAAJ&hl=es" class="author-link" style="color:inherit; text-decoration:none;">Stewart R. Santos-Arce</a>
    </p>
    <p>University of Guadalajara - LAEDC 2025</p>
  </header>

  <nav id="main-nav">
    <button id="nav-toggle" aria-label="Toggle navigation" style="display:none; align-items:center; gap:8px; background:#004080; color:#fff; border:none; border-radius:6px; padding:10px 18px; font-size:1.08rem; font-weight:500; cursor:pointer; margin:10px auto;">
      <span style="font-size:1.1rem;">Sections</span>
      <svg width="24" height="24" viewBox="0 0 24 24" fill="none" style="vertical-align:middle;" xmlns="http://www.w3.org/2000/svg">
        <rect x="4" y="7" width="16" height="2" rx="1" fill="#fff"/>
        <rect x="4" y="11" width="16" height="2" rx="1" fill="#fff"/>
        <rect x="4" y="15" width="16" height="2" rx="1" fill="#fff"/>
      </svg>
    </button>
    <div id="nav-links">
      <a href="#abstract">Abstract</a>
      <a href="#introduction">Introduction</a>
      <a href="#methods">Materials & Methods</a>
      <a href="#results">Results</a>
      <a href="#conclusions">Conclusions</a>
      <a href="#references">References</a>
    </div>
  </nav>

  <main class="poster-grid">
    <section id="abstract">
      <h2>Abstract</h2>
      <p>
        Amyotrophic Lateral Sclerosis (ALS) is a progressive neurodegenerative condition that deteriorates motor neurons, leading to muscle 
        weakness and impairments in voluntary control of movement, speech, and facial expression. Current diagnostic methods, based on clinical 
        evaluations and specialized tests, present significant delays, affecting patient survival and quality of life. This study proposes 
        a non-invasive method to detect ALS by characterizing facial markers in the frequency domain through machine learning algorithms.

      </p>
    </section>

    <section id="introduction">
      <h2>Introduction</h2>
      <p>
        Amyotrophic Lateral Sclerosis (ALS) is a neurodegenerative disease that affects motor neurons, causing progressive muscle weakness
         and loss of voluntary motor control [1]. This directly impacts critical functions such as speech and facial expressions [1]. 
         Current diagnostic methods, based on clinical evaluations and specialized testing (electromyography, MRI) have a 10~16 months 
         delay, which hinders early interventions and reduces patient survival.
        <div class="intro-figure">
          <img src="introduction.png" alt="Current diagnostic methods" class="poster-img">
          <figcaption class="intro-caption">
            Figure 1. Delayed diagnosis of ALS impacts patients' life expectancy.
          </figcaption>
        </div>
        Faced with this challenge, there is a need to develop accessible and non-invasive tools for early diagnosis. Recent studies [2], 
        [3] have explored facial motion analysis using Machine Learning (ML), demonstrating its potential. However, these studies have 
        only focused on time-domain analysis. This work proposes an innovative method that combines frequency-domain facial signal 
        processing with classification algorithms, offering a complementary alternative to traditional approaches.

      </p>
    </section>

    <section id="methods">
      <h2>Materials and Methods</h2>
      <p>
       The Toronto NeuroFace dataset [4] is used for this study,  which contains video recordings of 22 participants (11 with ALS 
       confirmed diagnosis and 11 healthy controls) performing nine orofacial diadochokinesia tasks [4]. Using the MediaPipe® framework, 
       440 facial markers were initially extracted for each video frame. These signals were transformed into polar coordinates (radius 
       r and angle θ), taking the nasal tip as the reference origin.
      </p>


      <div class="video-grid">
        <div class="video-cell">
          <span class="video-label">a)</span>
          <video controls muted>
            <source src="mesh-.mp4" type="video/mp4">
            Tu navegador no soporta el video.
          </video>
        </div>
        <div class="video-cell">
          <span class="video-label">b)</span>
          <video controls muted>
            <source src="points-.mp4" type="video/mp4">
            Tu navegador no soporta el video.
          </video>
        </div>
        <div class="video-cell">
          <span class="video-label">c)</span>
          <video controls muted>
            <source src="radios-.mp4" type="video/mp4">
            Tu navegador no soporta el video.
          </video>
        </div>
        <div class="video-cell">
          <span class="video-label">d)</span>
          <video controls muted>
            <source src="thetas-.mp4" type="video/mp4">
            Tu navegador no soporta el video.
          </video>
        </div>
      </div>
      <figcaption class="video-caption">
        Figure 2: Landmark extraction from videos. a) Face mesh estimated by MediaPipe, b) Calculation of radius and theta signals, c) Example of radius signal, d) Example of theta signal
      </figcaption>

        <p>
          For spectral analysis, coherence was computed in the 0.1–5 Hz band between pairs of symmetrical bilateral markers, a range that 
          encompasses the characteristic frequencies of orofacial movements during speech. Subsequently, the set was reduced to 24 marker 
          pairs per task, using a feature importance analysis, selecting those with the greatest discriminatory power between groups.
          Six ML architectures were implemented for binary classification (ALS vs. control): Support Vector Machine, Random Forest, Decision 
          Trees, Multilayer Neural Networks, K-Nearest Neighbors, and Logistic Regression. Each model was independently evaluated for the nine 
          diadochokinesis tasks [4] using cross-validation, reporting performance metrics such as accuracy, sensitivity, and specificity.
            <div class="figure3">
              <img src="coherence.png" alt="Coherence analysis" class="poster-img">
              <figcaption class="intro-caption">
                Figure 3. Processing of facial markers: a) Extraction of the initial 440 markers, b) Coherence calculation between symmetric signals, c) Markers reduction.
              </figcaption>
            </div>
        </p>
    </section>

    <section id="results">
      <h2>Results and Discussion</h2>
      <p>
        Comparative analysis of the ML algorithms revealed differentiated performance across the facial tasks evaluated. The highest average accuracy was obtained on the BIGSMILE task (86%), followed by PA (71%) and PATAKA (66%).
        <div class="intro-figure">
          <img src="results.png" alt="Results" class="poster-img">
          <figcaption class="intro-caption">
            Figure 4. Accuracy results for each task, using frequency analysis.
          </figcaption>
        </div>
        Among the algorithms evaluated, K-Nearest Neighbors and Vector Machine stood out for their high accuracy
         (up to 90% in BIGSMILE). The remaining algorithms showed moderate accuracy. This variability in performance 
         suggests that selecting specific tasks and appropriate algorithms is crucial for optimizing ALS detection through 
         facial analysis.
      </p>
    </section>

    <section id="conclusions">
      <h2>Conclusions</h2>
      <p>
        This study demonstrates that frequency analysis of orofacial movements using ML algorithms can distinguish between ALS patients 
        and healthy controls. The accuracy analysis suggests that the BIGSMILE task could increase the efficiency of ALS proper diagnosis.

      </p>
    </section>

    <section id="references" class="references">
      <h2>References</h2>
      <ol>
        <li style="margin-bottom: 1em;">D. Richards, J. A. Morren, and E. P. Pioro, <a href="https://www.sciencedirect.com/science/article/pii/S0022510X20303919" style="color:#0040ff; text-decoration:underline;">“Time to diagnosis and factors affecting diagnostic delay in amyotrophic lateral sclerosis”</a>, Oct. 2020.</li>
        <li style="margin-bottom: 1em;">A. Bandini, J. R. Green, B. Taati, S. Orlandi, L. Zinman, and Y. Yunusova, <a href="https://ieeexplore.ieee.org/abstract/document/8373824" style="color:#0040ff; text-decoration:underline;">“Automatic Detection of Amyotrophic Lateral Sclerosis (ALS) from Video-Based Analysis of Facial Movements: Speech and Non-Speech Tasks”</a>, May 2018.</li>
        <li style="margin-bottom: 1em;">N. Gomes, A. Yoshida, M. Roder, G. Camargo De Oliveira, and J. Papa, <a href="https://ui.adsabs.harvard.edu/abs/2023arXiv230712159B/abstract" style="color:#0040ff; text-decoration:underline;">“Facial Point Graphs for Amyotrophic Lateral Sclerosis Identification”</a>, 2024.</li>
        <li style="margin-bottom: 1em;">A. Bandini et al., <a href="https://ieeexplore.ieee.org/abstract/document/9177259" style="color:#0040ff; text-decoration:underline;">“A New Dataset for Facial Motion Analysis in Individuals With Neurological Disorders”</a>, Apr 2021.</li>
      </ol>
    </section>

    <section id="download" style="text-align:center;">
      <a href="https://drive.google.com/file/d/1Al7MT6a9txmScvWruUXXNZ6P0bnWBmbN/view?usp=sharing" target="_blank" class="download-btn" style="display:inline-flex; align-items:center; gap:8px; background:#004080; color:#fff; border-radius:6px; padding:8px 18px; font-size:1.08rem; font-weight:500; text-decoration:none; transition:background 0.2s;">
        Download PDF
        <img src="pdf.svg" alt="Download PDF" style="height:24px; width:auto; vertical-align:middle;">
      </a>
    </section>
  </main>

  <footer>
    <p style="text-align:center;">&copy; 2025 IEEE LAEDC</p>
    <p style="text-align:center;">
      <a href="https://www.linkedin.com/in/alejandrodiazmonstesdeoca/" class="designer-link" style="color:inherit; text-decoration:none;">Designed by Alejandro Díaz Montes de Oca</a>
    </p>
    <div style="display:flex; justify-content:center; align-items:center; gap:20px; margin-top:10px;">
      <img src="EDS.png" alt="EDS logo" style="max-width:120px; height:auto;">
      <img src="UDG.png" alt="UDG logo" style="max-width:120px; height:auto;">
    </div>
  </footer>

  <script src="script.js"></script>
</body>
</html>
